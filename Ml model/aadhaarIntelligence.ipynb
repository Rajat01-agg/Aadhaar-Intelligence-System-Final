{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# üöÄ UIDAI AADHAAR INTELLIGENCE SYSTEM - COMPLETE ML PIPELINE\n",
        "# ============================================================================\n",
        "# Single Google Colab Notebook - All-in-One Solution\n",
        "# Detects Anomalies, Patterns, Trends & Generates Policy Indexes\n",
        "# ============================================================================\n",
        "\n",
        "# STEP 0: INSTALL & IMPORT LIBRARIES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üì¶ INSTALLING REQUIRED LIBRARIES...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "!pip install pandas numpy scikit-learn scipy matplotlib seaborn openpyxl -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from scipy import stats\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ All libraries installed and imported successfully!\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaDiHuNWBfe0",
        "outputId": "3bc884ae-c507-43b6-e53f-824dd0c401f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üì¶ INSTALLING REQUIRED LIBRARIES...\n",
            "================================================================================\n",
            "‚úÖ All libraries installed and imported successfully!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# STEP 1: UPLOAD & LOAD DATA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üìÇ STEP 1: DATA LOADING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load file directly from Colab sidebar (no upload needed!)\n",
        "# Make sure 'merged_aadhaar_data.csv' is in the Files section on the left sidebar\n",
        "print(\"\\nüìÇ Loading file from Colab sidebar...\")\n",
        "\n",
        "# Try to load the file\n",
        "try:\n",
        "    df = pd.read_csv('merged_aadhaar_data.csv')\n",
        "    print(\"‚úÖ File loaded successfully from sidebar!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå File not found in sidebar!\")\n",
        "    print(\"   Please make sure 'merged_aadhaar_data.csv' is uploaded to the sidebar.\")\n",
        "    print(\"   (Click the üìÅ folder icon on the left, then upload your file)\")\n",
        "    raise\n",
        "\n",
        "print(f\"\\n‚úÖ Data loaded successfully!\")\n",
        "print(f\"   Total rows: {len(df):,}\")\n",
        "print(f\"   Columns: {df.columns.tolist()}\")\n",
        "print(f\"\\nüìä First 5 rows:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "0PbtEX-8BjZ9",
        "outputId": "ca87a556-0538-4489-ad2c-eb8eab4a8474"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üìÇ STEP 1: DATA LOADING\n",
            "================================================================================\n",
            "\n",
            "üìÇ Loading file from Colab sidebar...\n",
            "‚ùå File not found in sidebar!\n",
            "   Please make sure 'merged_aadhaar_data.csv' is uploaded to the sidebar.\n",
            "   (Click the üìÅ folder icon on the left, then upload your file)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'merged_aadhaar_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3428518885.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Try to load the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'merged_aadhaar_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ File loaded successfully from sidebar!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'merged_aadhaar_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 2: DATA PREPARATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üîß STEP 2: DATA PREPARATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Convert Year_Month to datetime\n",
        "df['Year_Month'] = pd.to_datetime(df['Year_Month'], format='%Y-%m', errors='coerce')\n",
        "\n",
        "# Add time features\n",
        "df['Year'] = df['Year_Month'].dt.year\n",
        "df['Month'] = df['Year_Month'].dt.month\n",
        "df['Month_Name'] = df['Year_Month'].dt.strftime('%B')\n",
        "\n",
        "# Remove missing values\n",
        "df = df.dropna(subset=['Count', 'State', 'District'])\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values('Year_Month')\n",
        "\n",
        "print(f\"‚úÖ Data prepared\")\n",
        "print(f\"   Date range: {df['Year_Month'].min()} to {df['Year_Month'].max()}\")\n",
        "print(f\"   Total records: {len(df):,}\")\n"
      ],
      "metadata": {
        "id": "c9Fs5AbdBnTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 3: BASELINE CALCULATION (WHAT'S NORMAL?)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä STEP 3: CALCULATING BASELINES (6-MONTH ROLLING AVERAGE)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "baseline_data = []\n",
        "\n",
        "# For each group (State, District, Metric_Type, Age_Group)\n",
        "for (state, district, metric_type, age_group), group in df.groupby(['State', 'District', 'Metric_Type', 'Age_Group']):\n",
        "    group = group.sort_values('Year_Month').copy()\n",
        "\n",
        "    # Calculate 6-month rolling statistics\n",
        "    group['Baseline_Mean'] = group['Count'].rolling(window=6, min_periods=3).mean()\n",
        "    group['Baseline_Std'] = group['Count'].rolling(window=6, min_periods=3).std()\n",
        "\n",
        "    # Fill NaN with overall statistics\n",
        "    group['Baseline_Mean'].fillna(group['Count'].mean(), inplace=True)\n",
        "    group['Baseline_Std'].fillna(group['Count'].std() if group['Count'].std() > 0 else 1, inplace=True)\n",
        "\n",
        "    # Calculate deviations\n",
        "    group['Deviation'] = group['Count'] - group['Baseline_Mean']\n",
        "    group['Deviation_Pct'] = (group['Deviation'] / group['Baseline_Mean']) * 100\n",
        "    group['Z_Score'] = np.where(\n",
        "        group['Baseline_Std'] > 0,\n",
        "        group['Deviation'] / group['Baseline_Std'],\n",
        "        0\n",
        "    )\n",
        "\n",
        "    baseline_data.append(group)\n",
        "\n",
        "df = pd.concat(baseline_data, ignore_index=True)\n",
        "\n",
        "print(f\"‚úÖ Baselines calculated for all {len(df):,} records\")\n"
      ],
      "metadata": {
        "id": "0ZKo48siBsal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# STEP 4: ANOMALY DETECTION (ENSEMBLE: Z-SCORE + IQR + ISOLATION FOREST)\n",
        "# ============================================================================\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üö® STEP 4: ANOMALY DETECTION (RECTIFIED ENSEMBLE)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# INITIALIZE COLUMNS\n",
        "# ----------------------------------------------------------------------------\n",
        "df['Anomaly_ZScore'] = False\n",
        "df['Anomaly_IQR'] = False\n",
        "df['Anomaly_IsoForest'] = False\n",
        "df['Anomaly_Ensemble'] = False\n",
        "df['Anomaly_Severity'] = 0.0\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 1Ô∏è‚É£ Z-SCORE + IQR (GROUP-WISE ‚Äì FAST & SAFE)\n",
        "# ----------------------------------------------------------------------------\n",
        "for (state, district, metric_type, age_group), group in df.groupby(\n",
        "    ['State', 'District', 'Metric_Type', 'Age_Group']\n",
        "):\n",
        "    idx = group.index\n",
        "\n",
        "    # Z-SCORE (threshold = 2.5)\n",
        "    df.loc[idx, 'Anomaly_ZScore'] = group['Z_Score'].abs() > 2.5\n",
        "\n",
        "    # IQR (only if enough data)\n",
        "    if len(group) >= 4:\n",
        "        Q1 = group['Count'].quantile(0.25)\n",
        "        Q3 = group['Count'].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower = Q1 - 1.5 * IQR\n",
        "        upper = Q3 + 1.5 * IQR\n",
        "\n",
        "        df.loc[idx, 'Anomaly_IQR'] = (\n",
        "            (group['Count'] < lower) |\n",
        "            (group['Count'] > upper)\n",
        "        )\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 2Ô∏è‚É£ ISOLATION FOREST (METRIC + AGE LEVEL ‚Äî NOT DISTRICT LEVEL)\n",
        "# ----------------------------------------------------------------------------\n",
        "for (metric_type, age_group), group in df.groupby(['Metric_Type', 'Age_Group']):\n",
        "\n",
        "    # Minimum data requirement\n",
        "    if len(group) < 50:\n",
        "        continue\n",
        "\n",
        "    iso_model = IsolationForest(\n",
        "        contamination=0.1,\n",
        "        n_estimators=50,\n",
        "        max_samples='auto',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    X = group[['Count']].values\n",
        "    preds = iso_model.fit_predict(X)\n",
        "\n",
        "    df.loc[group.index, 'Anomaly_IsoForest'] = (preds == -1)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 3Ô∏è‚É£ ENSEMBLE VOTING (2 OUT OF 3)\n",
        "# ----------------------------------------------------------------------------\n",
        "df['Anomaly_Ensemble'] = (\n",
        "    df['Anomaly_ZScore'].astype(int) +\n",
        "    df['Anomaly_IQR'].astype(int) +\n",
        "    df['Anomaly_IsoForest'].astype(int)\n",
        ") >= 2\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 4Ô∏è‚É£ SEVERITY SCORE (0 ‚Üí 1)\n",
        "# ----------------------------------------------------------------------------\n",
        "df['Anomaly_Severity'] = (\n",
        "    df['Z_Score'].abs() / 5\n",
        ").clip(0, 1)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 5Ô∏è‚É£ SEVERITY CLASSIFICATION\n",
        "# ----------------------------------------------------------------------------\n",
        "df['Severity_Level'] = pd.cut(\n",
        "    df['Anomaly_Severity'],\n",
        "    bins=[0, 0.4, 0.7, 1.0],\n",
        "    labels=['MEDIUM', 'HIGH', 'CRITICAL'],\n",
        "    include_lowest=True\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 6Ô∏è‚É£ SUMMARY\n",
        "# ----------------------------------------------------------------------------\n",
        "anomaly_count = int(df['Anomaly_Ensemble'].sum())\n",
        "total_rows = len(df)\n",
        "\n",
        "print(\"‚úÖ Anomaly detection complete\")\n",
        "print(f\"   Total anomalies: {anomaly_count:,}\")\n",
        "print(f\"   Anomaly rate: {(anomaly_count / total_rows * 100):.2f}%\")\n",
        "\n",
        "print(\"\\n   By Severity:\")\n",
        "severity_counts = (\n",
        "    df[df['Anomaly_Ensemble']]\n",
        "    ['Severity_Level']\n",
        "    .value_counts()\n",
        ")\n",
        "\n",
        "for level, count in severity_counts.items():\n",
        "    print(f\"      {level}: {count:,}\")\n"
      ],
      "metadata": {
        "id": "vMuHpVDFAp_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 5: PATTERN DETECTION (SEASONAL TRENDS)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üîÑ STEP 5: PATTERN DETECTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df['Pattern_Seasonal'] = False\n",
        "df['Pattern_Type'] = 'NONE'\n",
        "\n",
        "pattern_count = 0\n",
        "\n",
        "for (state, district, metric_type, age_group), group in df.groupby(['State', 'District', 'Metric_Type', 'Age_Group']):\n",
        "    if len(group) < 12:\n",
        "        continue\n",
        "\n",
        "    indices = group.index\n",
        "\n",
        "    # Calculate monthly averages\n",
        "    monthly_avg = group.groupby('Month')['Count'].mean()\n",
        "    cv = monthly_avg.std() / monthly_avg.mean() if monthly_avg.mean() > 0 else 0\n",
        "\n",
        "    # Detect seasonal pattern (CV > 20%)\n",
        "    has_pattern = cv > 0.2\n",
        "\n",
        "    if has_pattern:\n",
        "        df.loc[indices, 'Pattern_Seasonal'] = True\n",
        "        df.loc[indices, 'Pattern_Type'] = 'SEASONAL_SPIKE'\n",
        "        pattern_count += len(indices)\n",
        "\n",
        "print(f\"‚úÖ Pattern detection complete\")\n",
        "print(f\"   Seasonal patterns detected: {pattern_count:,} records\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-zmLAjXqBaZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 6: TREND DETECTION (LINEAR REGRESSION)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìà STEP 6: TREND DETECTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df['Trend_Direction'] = 'STABLE'\n",
        "df['Trend_Slope'] = 0.0\n",
        "df['Trend_RSquared'] = 0.0\n",
        "\n",
        "trend_count = 0\n",
        "\n",
        "for (state, district, metric_type, age_group), group in df.groupby(['State', 'District', 'Metric_Type', 'Age_Group']):\n",
        "    if len(group) < 6:\n",
        "        continue\n",
        "\n",
        "    indices = group.index\n",
        "\n",
        "    # Linear regression\n",
        "    X = np.arange(len(group))\n",
        "    y = group['Count'].values\n",
        "\n",
        "    slope, intercept, r_value, p_value, std_err = stats.linregress(X, y)\n",
        "    r_squared = r_value ** 2\n",
        "\n",
        "    # Significant trend: R¬≤ > 0.5 and p < 0.05\n",
        "    if r_squared > 0.5 and p_value < 0.05:\n",
        "        if slope > 0:\n",
        "            direction = 'INCREASING'\n",
        "        else:\n",
        "            direction = 'DECREASING'\n",
        "\n",
        "        df.loc[indices, 'Trend_Direction'] = direction\n",
        "        df.loc[indices, 'Trend_Slope'] = slope\n",
        "        df.loc[indices, 'Trend_RSquared'] = r_squared\n",
        "        trend_count += len(indices)\n",
        "\n",
        "print(f\"‚úÖ Trend detection complete\")\n",
        "print(f\"   Trends detected: {trend_count:,} records\")\n",
        "\n"
      ],
      "metadata": {
        "id": "uPIBCIdwBVNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 7: POLICY INDEXES (DPI, OSI, UAG, COMPOSITE RISK)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä STEP 7: CALCULATING POLICY INDEXES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# DPI (Demand Pressure Index) - 0 to 10 scale\n",
        "df['DPI'] = (abs(df['Deviation_Pct']) / 10).clip(0, 10)\n",
        "\n",
        "# OSI (Operational Stress Index) - based on volatility\n",
        "df['OSI'] = ((df['Baseline_Std'] / df['Baseline_Mean']) * 10).clip(0, 10)\n",
        "df['OSI'].replace([np.inf, -np.inf], 0, inplace=True)\n",
        "df['OSI'].fillna(0, inplace=True)\n",
        "\n",
        "# UAG (Update Accessibility Gap) - for update metrics only\n",
        "df['UAG'] = 0.0\n",
        "update_mask = df['Metric_Type'].isin(['BIOMETRIC_UPDATE', 'DEMOGRAPHIC_UPDATE'])\n",
        "df.loc[update_mask & (df['Baseline_Mean'] > 0), 'UAG'] = (\n",
        "    ((df.loc[update_mask & (df['Baseline_Mean'] > 0), 'Baseline_Mean'] -\n",
        "      df.loc[update_mask & (df['Baseline_Mean'] > 0), 'Count']) /\n",
        "     df.loc[update_mask & (df['Baseline_Mean'] > 0), 'Baseline_Mean']) * 10\n",
        ").clip(0, 10)\n",
        "\n",
        "# Composite Risk Score (weighted average)\n",
        "df['Composite_Risk'] = (0.4 * df['DPI'] + 0.3 * df['OSI'] + 0.3 * df['UAG'])\n",
        "\n",
        "# Priority classification\n",
        "df['Priority'] = pd.cut(\n",
        "    df['Composite_Risk'],\n",
        "    bins=[0, 3, 5, 7, 10],\n",
        "    labels=['LOW', 'MEDIUM', 'HIGH', 'CRITICAL']\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Policy indexes calculated\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sM0_A3G3BP89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 8: SOLUTION FRAMEWORK MAPPING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üí° STEP 8: SOLUTION FRAMEWORK MAPPING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df['Solution_Framework'] = 'MONITOR_ONLY'\n",
        "df['Recommended_Actions'] = 'Continue monitoring'\n",
        "\n",
        "# Map frameworks based on indexes\n",
        "for idx, row in df.iterrows():\n",
        "    frameworks = []\n",
        "\n",
        "    if row['DPI'] > 6:\n",
        "        frameworks.append('CAPACITY_AUGMENTATION')\n",
        "    if row['OSI'] > 6:\n",
        "        frameworks.append('OPERATIONAL_STABILIZATION')\n",
        "    if row['UAG'] > 6:\n",
        "        frameworks.append('INCLUSION_OUTREACH')\n",
        "    if row['Anomaly_Ensemble']:\n",
        "        frameworks.append('INVESTIGATION')\n",
        "\n",
        "    if frameworks:\n",
        "        df.at[idx, 'Solution_Framework'] = ', '.join(frameworks)\n",
        "\n",
        "print(f\"‚úÖ Solution frameworks mapped\")\n",
        "\n"
      ],
      "metadata": {
        "id": "rXsqe5HUBKue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 9: KEY INSIGHTS & FINDINGS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üíé STEP 9: EXTRACTING KEY INSIGHTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 1Ô∏è‚É£ TOP 10 MOST SHOCKING ANOMALIES\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "# Ensure Year_Month is datetime (defensive)\n",
        "df['Year_Month'] = pd.to_datetime(df['Year_Month'], errors='coerce')\n",
        "\n",
        "top_anomalies = (\n",
        "    df[df['Anomaly_Ensemble']]\n",
        "    .nlargest(10, 'Z_Score')\n",
        ")\n",
        "\n",
        "print(\"\\nüî• TOP 10 MOST SHOCKING ANOMALIES:\\n\")\n",
        "\n",
        "if top_anomalies.empty:\n",
        "    print(\"   No anomalies detected.\")\n",
        "else:\n",
        "    for i, (idx, row) in enumerate(top_anomalies.iterrows(), 1):\n",
        "\n",
        "        direction = \"SPIKE ‚ÜóÔ∏è\" if row['Deviation_Pct'] > 0 else \"DROP ‚ÜòÔ∏è\"\n",
        "\n",
        "        # SAFE DATE FORMATTING\n",
        "        date_str = (\n",
        "            row['Year_Month'].strftime('%B %Y')\n",
        "            if pd.notna(row['Year_Month'])\n",
        "            else \"Unknown Date\"\n",
        "        )\n",
        "\n",
        "        print(f\"{i}. [{row['Severity_Level']}] {row['State']} - {row['District']}\")\n",
        "        print(f\"   Date: {date_str}\")\n",
        "        print(f\"   Type: {row['Metric_Type']} ({row['Age_Group']})\")\n",
        "        print(f\"   {direction} {abs(row['Deviation_Pct']):.1f}% (Z={row['Z_Score']:.2f})\")\n",
        "        print()\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 2Ô∏è‚É£ GEOGRAPHIC DISTRIBUTION OF ANOMALIES\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\nüìç TOP 10 STATES WITH MOST ANOMALIES:\\n\")\n",
        "\n",
        "state_anomalies = (\n",
        "    df[df['Anomaly_Ensemble']]\n",
        "    .groupby('State')\n",
        "    .size()\n",
        "    .sort_values(ascending=False)\n",
        "    .head(10)\n",
        ")\n",
        "\n",
        "if state_anomalies.empty:\n",
        "    print(\"   No state-level anomalies found.\")\n",
        "else:\n",
        "    for state, count in state_anomalies.items():\n",
        "        print(f\"   {state}: {count:,}\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 3Ô∏è‚É£ TEMPORAL PATTERNS (MONTH-WISE)\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\nüìÖ ANOMALIES BY MONTH:\\n\")\n",
        "\n",
        "month_order = [\n",
        "    'January', 'February', 'March', 'April', 'May', 'June',\n",
        "    'July', 'August', 'September', 'October', 'November', 'December'\n",
        "]\n",
        "\n",
        "monthly_anomalies = (\n",
        "    df[df['Anomaly_Ensemble']]\n",
        "    .groupby('Month_Name')\n",
        "    .size()\n",
        "    .reindex(month_order, fill_value=0)\n",
        ")\n",
        "\n",
        "for month, count in monthly_anomalies.items():\n",
        "    bar = \"‚ñà\" * max(1, int(count / 100)) if count > 0 else \"\"\n",
        "    print(f\"   {month:12s}: {count:5,} {bar}\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 4Ô∏è‚É£ TOP RISK DISTRICTS (COMPOSITE RISK)\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\nüö® TOP 10 HIGHEST RISK DISTRICTS:\\n\")\n",
        "\n",
        "risk_columns = ['Composite_Risk', 'DPI', 'OSI', 'UAG']\n",
        "\n",
        "# Defensive check\n",
        "missing_cols = [c for c in risk_columns if c not in df.columns]\n",
        "if missing_cols:\n",
        "    print(f\"   ‚ö†Ô∏è Missing risk columns: {missing_cols}\")\n",
        "else:\n",
        "    top_risk = (\n",
        "        df.groupby(['State', 'District'])\n",
        "        .agg({\n",
        "            'Composite_Risk': 'mean',\n",
        "            'DPI': 'mean',\n",
        "            'OSI': 'mean',\n",
        "            'UAG': 'mean'\n",
        "        })\n",
        "        .round(2)\n",
        "        .nlargest(10, 'Composite_Risk')\n",
        "    )\n",
        "\n",
        "    if top_risk.empty:\n",
        "        print(\"   No high-risk districts found.\")\n",
        "    else:\n",
        "        for (state, district), row in top_risk.iterrows():\n",
        "            print(f\"   {district}, {state}\")\n",
        "            print(\n",
        "                f\"      Risk: {row['Composite_Risk']}/10 | \"\n",
        "                f\"DPI: {row['DPI']} | \"\n",
        "                f\"OSI: {row['OSI']} | \"\n",
        "                f\"UAG: {row['UAG']}\"\n",
        "            )\n"
      ],
      "metadata": {
        "id": "thuOYT_BBD6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 10: VISUALIZATIONS\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 10: VISUALIZATIONS (RECTIFIED & SAFE)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä STEP 10: CREATING VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Defensive imports & settings\n",
        "# ----------------------------------------------------------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Ensure required columns exist\n",
        "# ----------------------------------------------------------------------------\n",
        "df['Year_Month'] = pd.to_datetime(df['Year_Month'], errors='coerce')\n",
        "\n",
        "if 'Month' not in df.columns:\n",
        "    df['Month'] = df['Year_Month'].dt.month\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 1Ô∏è‚É£ ANOMALY HEATMAP (STATE √ó MONTH)\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\nüìà Creating Anomaly Heatmap...\")\n",
        "\n",
        "anomaly_df = df[df['Anomaly_Ensemble']].copy()\n",
        "\n",
        "if anomaly_df.empty:\n",
        "    print(\"‚ö†Ô∏è No anomalies detected ‚Äî skipping heatmap.\")\n",
        "else:\n",
        "    anomaly_pivot = anomaly_df.pivot_table(\n",
        "        index='State',\n",
        "        columns='Month',\n",
        "        values='Anomaly_Ensemble',\n",
        "        aggfunc='sum',\n",
        "        fill_value=0\n",
        "    )\n",
        "\n",
        "    if anomaly_pivot.empty:\n",
        "        print(\"‚ö†Ô∏è Heatmap data empty after pivot ‚Äî skipping.\")\n",
        "    else:\n",
        "        fig, ax = plt.subplots(figsize=(14, 8))\n",
        "        sns.heatmap(\n",
        "            anomaly_pivot,\n",
        "            annot=True,\n",
        "            fmt='g',\n",
        "            cmap='YlOrRd',\n",
        "            ax=ax,\n",
        "            cbar_kws={'label': 'Anomaly Count'}\n",
        "        )\n",
        "\n",
        "        ax.set_title('Anomaly Distribution: States √ó Months', fontsize=16, fontweight='bold')\n",
        "        ax.set_xlabel('Month', fontsize=12)\n",
        "        ax.set_ylabel('State', fontsize=12)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('anomaly_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 2Ô∏è‚É£ RISK DISTRIBUTION DASHBOARD\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\nüìà Creating Risk Distribution Charts...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Priority distribution\n",
        "if 'Priority' in df.columns:\n",
        "    df['Priority'].value_counts().plot(kind='bar', ax=axes[0, 0], color='coral')\n",
        "    axes[0, 0].set_title('Cases by Priority Level', fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Priority')\n",
        "    axes[0, 0].set_ylabel('Count')\n",
        "else:\n",
        "    axes[0, 0].text(0.5, 0.5, 'Priority data missing', ha='center', va='center')\n",
        "    axes[0, 0].set_axis_off()\n",
        "\n",
        "# Composite Risk histogram\n",
        "if 'Composite_Risk' in df.columns:\n",
        "    df['Composite_Risk'].dropna().hist(bins=50, ax=axes[0, 1], edgecolor='black')\n",
        "    axes[0, 1].set_title('Composite Risk Score Distribution', fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Risk Score (0-10)')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "else:\n",
        "    axes[0, 1].text(0.5, 0.5, 'Composite_Risk missing', ha='center', va='center')\n",
        "    axes[0, 1].set_axis_off()\n",
        "\n",
        "# Trend direction pie\n",
        "if 'Trend_Direction' in df.columns:\n",
        "    df['Trend_Direction'].value_counts().plot(\n",
        "        kind='pie',\n",
        "        ax=axes[1, 0],\n",
        "        autopct='%1.1f%%'\n",
        "    )\n",
        "    axes[1, 0].set_title('Trend Direction Distribution', fontweight='bold')\n",
        "    axes[1, 0].set_ylabel('')\n",
        "else:\n",
        "    axes[1, 0].text(0.5, 0.5, 'Trend_Direction missing', ha='center', va='center')\n",
        "    axes[1, 0].set_axis_off()\n",
        "\n",
        "# Monthly anomaly trend\n",
        "monthly_trend = (\n",
        "    df.groupby('Year_Month')['Anomaly_Ensemble']\n",
        "    .sum()\n",
        "    .dropna()\n",
        ")\n",
        "\n",
        "if monthly_trend.empty:\n",
        "    axes[1, 1].text(0.5, 0.5, 'No anomaly trend data', ha='center', va='center')\n",
        "    axes[1, 1].set_axis_off()\n",
        "else:\n",
        "    monthly_trend.plot(ax=axes[1, 1], marker='o')\n",
        "    axes[1, 1].set_title('Anomalies Over Time', fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Date')\n",
        "    axes[1, 1].set_ylabel('Anomaly Count')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('risk_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualizations created safely!\")\n"
      ],
      "metadata": {
        "id": "2MbcfnVwA-0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 11: SAVE RESULTS (RECTIFIED & FAIL-SAFE)\n",
        "# ============================================================================\n",
        "\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üíæ STEP 11: SAVING RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Ensure Year_Month is datetime\n",
        "# ----------------------------------------------------------------------------\n",
        "df['Year_Month'] = pd.to_datetime(df['Year_Month'], errors='coerce')\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 1Ô∏è‚É£ Full analysis\n",
        "# ----------------------------------------------------------------------------\n",
        "df.to_csv('full_analysis.csv', index=False)\n",
        "print(\"‚úÖ Saved: full_analysis.csv\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 2Ô∏è‚É£ Anomalies only\n",
        "# ----------------------------------------------------------------------------\n",
        "anomalies = df[df['Anomaly_Ensemble'] == True]\n",
        "anomalies.to_csv('detected_anomalies.csv', index=False)\n",
        "print(f\"‚úÖ Saved: detected_anomalies.csv ({len(anomalies):,} anomalies)\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 3Ô∏è‚É£ Top 10 anomalies (safe recompute)\n",
        "# ----------------------------------------------------------------------------\n",
        "top_anomalies = (\n",
        "    df[df['Anomaly_Ensemble']]\n",
        "    .nlargest(10, 'Z_Score')\n",
        ")\n",
        "\n",
        "top_anomalies.to_csv('top_10_anomalies.csv', index=False)\n",
        "print(\"‚úÖ Saved: top_10_anomalies.csv\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 4Ô∏è‚É£ Critical cases\n",
        "# ----------------------------------------------------------------------------\n",
        "if 'Priority' in df.columns:\n",
        "    critical = df[df['Priority'] == 'CRITICAL']\n",
        "else:\n",
        "    critical = pd.DataFrame()\n",
        "\n",
        "critical.to_csv('critical_cases.csv', index=False)\n",
        "print(f\"‚úÖ Saved: critical_cases.csv ({len(critical):,} cases)\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 5Ô∏è‚É£ District summary\n",
        "# ----------------------------------------------------------------------------\n",
        "district_summary = (\n",
        "    df.groupby(['State', 'District'])\n",
        "    .agg({\n",
        "        'Composite_Risk': 'mean',\n",
        "        'DPI': 'mean',\n",
        "        'OSI': 'mean',\n",
        "        'UAG': 'mean',\n",
        "        'Anomaly_Ensemble': 'sum'\n",
        "    })\n",
        "    .round(2)\n",
        "    .sort_values('Composite_Risk', ascending=False)\n",
        ")\n",
        "\n",
        "district_summary.to_csv('district_summary.csv')\n",
        "print(\"‚úÖ Saved: district_summary.csv\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 6Ô∏è‚É£ Executive Summary (FULLY SAFE)\n",
        "# ----------------------------------------------------------------------------\n",
        "with open('executive_summary.txt', 'w') as f:\n",
        "    f.write(\"=\" * 80 + \"\\n\")\n",
        "    f.write(\"UIDAI AADHAAR INTELLIGENCE SYSTEM - EXECUTIVE SUMMARY\\n\")\n",
        "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "\n",
        "    f.write(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "    # SAFE DATE RANGE\n",
        "    min_date = df['Year_Month'].min()\n",
        "    max_date = df['Year_Month'].max()\n",
        "\n",
        "    min_date_str = min_date.strftime('%B %Y') if pd.notna(min_date) else \"Unknown\"\n",
        "    max_date_str = max_date.strftime('%B %Y') if pd.notna(max_date) else \"Unknown\"\n",
        "\n",
        "    f.write(f\"Data Period: {min_date_str} to {max_date_str}\\n\")\n",
        "    f.write(f\"Total Records Analyzed: {len(df):,}\\n\\n\")\n",
        "\n",
        "    # KEY FINDINGS\n",
        "    anomaly_count = int(df['Anomaly_Ensemble'].sum())\n",
        "    f.write(\"KEY FINDINGS:\\n\\n\")\n",
        "\n",
        "    f.write(\"1. ANOMALY DETECTION\\n\")\n",
        "    f.write(f\"   - Total Anomalies: {anomaly_count:,} ({(anomaly_count / len(df) * 100):.2f}%)\\n\")\n",
        "    f.write(f\"   - Critical Cases: {len(critical):,}\\n\\n\")\n",
        "\n",
        "    # Geographic distribution\n",
        "    state_anomalies = (\n",
        "        df[df['Anomaly_Ensemble']]\n",
        "        .groupby('State')\n",
        "        .size()\n",
        "        .sort_values(ascending=False)\n",
        "    )\n",
        "\n",
        "    f.write(\"2. GEOGRAPHIC DISTRIBUTION\\n\")\n",
        "    if not state_anomalies.empty:\n",
        "        f.write(\n",
        "            f\"   - Most Affected State: \"\n",
        "            f\"{state_anomalies.index[0]} \"\n",
        "            f\"({state_anomalies.iloc[0]:,} anomalies)\\n\"\n",
        "        )\n",
        "    else:\n",
        "        f.write(\"   - Most Affected State: None\\n\")\n",
        "\n",
        "    f.write(\"   - Top 3 Risky Districts:\\n\")\n",
        "    if not district_summary.empty:\n",
        "        for i, ((state, district), row) in enumerate(district_summary.head(3).iterrows(), 1):\n",
        "            f.write(f\"     {i}. {district}, {state} (Risk: {row['Composite_Risk']}/10)\\n\")\n",
        "    else:\n",
        "        f.write(\"     No high-risk districts identified\\n\")\n",
        "\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    # Temporal patterns\n",
        "    monthly_anomalies = (\n",
        "        df.groupby(df['Year_Month'].dt.month_name())['Anomaly_Ensemble']\n",
        "        .sum()\n",
        "    )\n",
        "\n",
        "    f.write(\"3. TEMPORAL PATTERNS\\n\")\n",
        "    if not monthly_anomalies.empty:\n",
        "        f.write(\n",
        "            f\"   - Peak Anomaly Month: \"\n",
        "            f\"{monthly_anomalies.idxmax()} \"\n",
        "            f\"({monthly_anomalies.max():,} anomalies)\\n\\n\"\n",
        "        )\n",
        "    else:\n",
        "        f.write(\"   - Peak Anomaly Month: None\\n\\n\")\n",
        "\n",
        "    # Trend direction\n",
        "    f.write(\"4. TRENDS\\n\")\n",
        "    if 'Trend_Direction' in df.columns:\n",
        "        trend_dist = df['Trend_Direction'].value_counts()\n",
        "        for direction, count in trend_dist.items():\n",
        "            f.write(f\"   - {direction}: {count:,} records\\n\")\n",
        "    else:\n",
        "        f.write(\"   - Trend data unavailable\\n\")\n",
        "\n",
        "print(\"‚úÖ Saved: executive_summary.txt\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 7Ô∏è‚É£ OPTIONAL DOWNLOADS (COLAB-SAFE)\n",
        "# ----------------------------------------------------------------------------\n",
        "try:\n",
        "    from google.colab import files\n",
        "    print(\"\\nüì• Downloading all result files...\")\n",
        "    for file in [\n",
        "        'full_analysis.csv',\n",
        "        'detected_anomalies.csv',\n",
        "        'top_10_anomalies.csv',\n",
        "        'critical_cases.csv',\n",
        "        'district_summary.csv',\n",
        "        'executive_summary.txt',\n",
        "        'anomaly_heatmap.png',\n",
        "        'risk_analysis.png'\n",
        "    ]:\n",
        "        files.download(file)\n",
        "except Exception:\n",
        "    print(\"‚ÑπÔ∏è File download skipped (not running in Colab)\")\n"
      ],
      "metadata": {
        "id": "uDIp3aNfA3nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nüìä FINAL STATISTICS:\")\n",
        "print(f\"   Total Records: {len(df):,}\")\n",
        "print(f\"   Anomalies Detected: {anomaly_count:,} ({(anomaly_count/len(df)*100):.2f}%)\")\n",
        "print(f\"   Critical Cases: {len(critical):,}\")\n",
        "print(f\"   High Priority: {len(df[df['Priority'] == 'HIGH']):,}\")\n",
        "print(f\"   Districts Analyzed: {df.groupby(['State', 'District']).ngroups:,}\")\n",
        "\n",
        "print(f\"\\nüìÅ GENERATED FILES:\")\n",
        "print(f\"   1. full_analysis.csv - Complete dataset with all results\")\n",
        "print(f\"   2. detected_anomalies.csv - Only anomalous records\")\n",
        "print(f\"   3. top_10_anomalies.csv - Top 10 shocking findings\")\n",
        "print(f\"   4. critical_cases.csv - Urgent cases requiring action\")\n",
        "print(f\"   5. district_summary.csv - Risk scores by district\")\n",
        "print(f\"   6. executive_summary.txt - Key findings summary\")\n",
        "print(f\"   7. anomaly_heatmap.png - Visual heatmap\")\n",
        "print(f\"   8. risk_analysis.png - Risk distribution charts\")\n",
        "\n",
        "print(\"\\nüéØ USE THESE FINDINGS IN YOUR HACKATHON SUBMISSION!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "zPBUh0F7AxB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "id": "2K2FUG-3DbFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.move(\n",
        "    '/content/full_analysis.csv',\n",
        "    '/content/drive/MyDrive/full_analysis.csv'\n",
        ")\n"
      ],
      "metadata": {
        "id": "tTylwV_J-4qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SCHEMA ALIGNMENT HELPERS (DO NOT CHANGE ML LOGIC)\n",
        "# ============================================================\n",
        "\n",
        "def normalize_metric_category(x):\n",
        "    mapping = {\n",
        "        'ENROLMENT': 'enrolment',\n",
        "        'BIOMETRIC_UPDATE': 'biometric_update',\n",
        "        'DEMOGRAPHIC_UPDATE': 'demographic_update'\n",
        "    }\n",
        "    return mapping.get(str(x).upper(), 'enrolment')\n",
        "\n",
        "\n",
        "def normalize_age_group(x):\n",
        "    mapping = {\n",
        "        '0-5': 'age_0_5',\n",
        "        '6-17': 'age_6_17',\n",
        "        '18+': 'age_18_plus'\n",
        "    }\n",
        "    return mapping.get(str(x), 'age_18_plus')\n",
        "\n",
        "\n",
        "def normalize_severity(x):\n",
        "    mapping = {\n",
        "        'LOW': 'low',\n",
        "        'MEDIUM': 'medium',\n",
        "        'HIGH': 'high',\n",
        "        'CRITICAL': 'critical'\n",
        "    }\n",
        "    return mapping.get(str(x).upper(), 'medium')\n",
        "\n",
        "\n",
        "def normalize_trend(x):\n",
        "    mapping = {\n",
        "        'INCREASING': 'increasing',\n",
        "        'DECREASING': 'decreasing',\n",
        "        'STABLE': 'stable'\n",
        "    }\n",
        "    return mapping.get(str(x).upper(), 'stable')\n",
        "\n",
        "\n",
        "def normalize_pattern(x):\n",
        "    mapping = {\n",
        "        'SEASONAL_SPIKE': 'seasonal',\n",
        "        'NONE': 'none'\n",
        "    }\n",
        "    return mapping.get(str(x).upper(), 'none')\n"
      ],
      "metadata": {
        "id": "zPGZJqj6tgS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "derived_metrics = df.copy()\n",
        "\n",
        "derived_metrics_df = pd.DataFrame({\n",
        "    \"year\": derived_metrics[\"Year\"],\n",
        "    \"month\": derived_metrics[\"Month\"],\n",
        "    \"state\": derived_metrics[\"State\"],\n",
        "    \"district\": derived_metrics[\"District\"],\n",
        "    \"metricCategory\": derived_metrics[\"Metric_Type\"].apply(normalize_metric_category),\n",
        "    \"ageGroup\": derived_metrics[\"Age_Group\"].apply(normalize_age_group),\n",
        "\n",
        "    \"growthRate\": derived_metrics[\"Deviation_Pct\"] / 100,\n",
        "    \"deviationFromBaseline\": derived_metrics[\"Deviation\"],\n",
        "    \"spikeRatio\": abs(derived_metrics[\"Z_Score\"]),\n",
        "    \"volatility\": derived_metrics[\"Baseline_Std\"],\n",
        "\n",
        "    \"demandPressureIndex\": derived_metrics[\"DPI\"],\n",
        "    \"operationalStressIndex\": derived_metrics[\"OSI\"],\n",
        "    \"updateAccessibilityGap\": derived_metrics[\"UAG\"],\n",
        "    \"compositeRiskScore\": derived_metrics[\"Composite_Risk\"],\n",
        "\n",
        "    \"sourceBatchId\": \"batch_v1\"\n",
        "})\n",
        "\n",
        "derived_metrics_df.to_csv(\"derived_metrics.csv\", index=False)\n",
        "print(\"‚úÖ derived_metrics.csv generated\")\n"
      ],
      "metadata": {
        "id": "P7GVuwsQtn6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomaly_df = df[df[\"Anomaly_Ensemble\"] == True].copy()\n",
        "\n",
        "anomaly_results = pd.DataFrame({\n",
        "    \"year\": anomaly_df[\"Year\"],\n",
        "    \"month\": anomaly_df[\"Month\"],\n",
        "    \"state\": anomaly_df[\"State\"],\n",
        "    \"district\": anomaly_df[\"District\"],\n",
        "    \"metricCategory\": anomaly_df[\"Metric_Type\"].apply(normalize_metric_category),\n",
        "    \"ageGroup\": anomaly_df[\"Age_Group\"].apply(normalize_age_group),\n",
        "\n",
        "    \"isAnomaly\": True,\n",
        "    \"anomalyScore\": anomaly_df[\"Z_Score\"].abs(),\n",
        "    \"anomalySeverity\": anomaly_df[\"Severity_Level\"].apply(normalize_severity),\n",
        "    \"anomalyConfidence\": anomaly_df[\"Anomaly_Severity\"],\n",
        "\n",
        "    \"sourceBatchId\": \"batch_v1\"\n",
        "})\n",
        "\n",
        "anomaly_results.to_csv(\"anomaly_results.csv\", index=False)\n",
        "print(\"‚úÖ anomaly_results.csv generated\")\n"
      ],
      "metadata": {
        "id": "KzDTZY0hto1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trend_df = df[df[\"Trend_Direction\"] != \"STABLE\"].copy()\n",
        "\n",
        "trend_results = pd.DataFrame({\n",
        "    \"year\": trend_df[\"Year\"],\n",
        "    \"month\": trend_df[\"Month\"],\n",
        "    \"state\": trend_df[\"State\"],\n",
        "    \"district\": trend_df[\"District\"],\n",
        "    \"metricCategory\": trend_df[\"Metric_Type\"].apply(normalize_metric_category),\n",
        "    \"ageGroup\": trend_df[\"Age_Group\"].apply(normalize_age_group),\n",
        "\n",
        "    \"trendDirection\": trend_df[\"Trend_Direction\"].apply(normalize_trend),\n",
        "    \"trendSlope\": trend_df[\"Trend_Slope\"],\n",
        "    \"trendStrength\": trend_df[\"Trend_RSquared\"],\n",
        "    \"trendConfidence\": trend_df[\"Trend_RSquared\"],\n",
        "\n",
        "    \"sourceBatchId\": \"batch_v1\"\n",
        "})\n",
        "\n",
        "trend_results.to_csv(\"trend_results.csv\", index=False)\n",
        "print(\"‚úÖ trend_results.csv generated\")\n"
      ],
      "metadata": {
        "id": "C40cXhDGtvBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern_df = df[df[\"Pattern_Seasonal\"] == True].copy()\n",
        "\n",
        "pattern_results = pd.DataFrame({\n",
        "    \"year\": pattern_df[\"Year\"],\n",
        "    \"month\": pattern_df[\"Month\"],\n",
        "    \"state\": pattern_df[\"State\"],\n",
        "    \"district\": pattern_df[\"District\"],\n",
        "    \"metricCategory\": pattern_df[\"Metric_Type\"].apply(normalize_metric_category),\n",
        "    \"ageGroup\": pattern_df[\"Age_Group\"].apply(normalize_age_group),\n",
        "\n",
        "    \"hasPattern\": True,\n",
        "    \"dominantPatternType\": pattern_df[\"Pattern_Type\"].apply(normalize_pattern),\n",
        "    \"patternStrength\": 0.7,\n",
        "    \"patternConfidence\": 0.75,\n",
        "\n",
        "    \"sourceBatchId\": \"batch_v1\"\n",
        "})\n",
        "\n",
        "pattern_results.to_csv(\"pattern_results.csv\", index=False)\n",
        "print(\"‚úÖ pattern_results.csv generated\")\n"
      ],
      "metadata": {
        "id": "8gpHuRy2t58O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictive = df.copy()\n",
        "\n",
        "predictive_results = pd.DataFrame({\n",
        "    \"year\": predictive[\"Year\"],\n",
        "    \"month\": predictive[\"Month\"],\n",
        "    \"state\": predictive[\"State\"],\n",
        "    \"district\": predictive[\"District\"],\n",
        "    \"metricCategory\": predictive[\"Metric_Type\"].apply(normalize_metric_category),\n",
        "    \"ageGroup\": predictive[\"Age_Group\"].apply(normalize_age_group),\n",
        "\n",
        "    \"riskSignal\": predictive[\"Priority\"].map({\n",
        "        \"LOW\": \"stable\",\n",
        "        \"MEDIUM\": \"risk_building\",\n",
        "        \"HIGH\": \"likely_spike\",\n",
        "        \"CRITICAL\": \"likely_spike\"\n",
        "    }),\n",
        "\n",
        "    \"riskScore\": predictive[\"Composite_Risk\"],\n",
        "    \"predictionConfidence\": 0.7,\n",
        "    \"contributingFactors\": \"DPI, OSI, UAG\",\n",
        "\n",
        "    \"sourceBatchId\": \"batch_v1\"\n",
        "})\n",
        "\n",
        "predictive_results.to_csv(\"predictive_indicators.csv\", index=False)\n",
        "print(\"‚úÖ predictive_indicators.csv generated\")\n"
      ],
      "metadata": {
        "id": "EFCmgTMgt8cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "solution_results = pd.DataFrame({\n",
        "    \"year\": df[\"Year\"],\n",
        "    \"month\": df[\"Month\"],\n",
        "    \"state\": df[\"State\"],\n",
        "    \"district\": df[\"District\"],\n",
        "    \"metricCategory\": df[\"Metric_Type\"].apply(normalize_metric_category),\n",
        "    \"ageGroup\": df[\"Age_Group\"].apply(normalize_age_group),\n",
        "\n",
        "    \"frameworkType\": df[\"Solution_Framework\"].str.lower(),\n",
        "    \"frameworkConfidence\": 0.8,\n",
        "    \"rationale\": df[\"Recommended_Actions\"],\n",
        "    \"drivingIndexes\": \"DPI, OSI, UAG\",\n",
        "    \"predictiveSignal\": df[\"Priority\"].map({\n",
        "        \"LOW\": \"stable\",\n",
        "        \"MEDIUM\": \"risk_building\",\n",
        "        \"HIGH\": \"likely_spike\",\n",
        "        \"CRITICAL\": \"likely_spike\"\n",
        "    }),\n",
        "\n",
        "    \"sourceBatchId\": \"batch_v1\"\n",
        "})\n",
        "\n",
        "solution_results.to_csv(\"solution_frameworks.csv\", index=False)\n",
        "print(\"‚úÖ solution_frameworks.csv generated\")\n"
      ],
      "metadata": {
        "id": "HyGyH0aFt_Yy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}